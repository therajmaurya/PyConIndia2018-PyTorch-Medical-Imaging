{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PyTorch Introduction.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"qIO3SZN4krvQ","colab_type":"text"},"cell_type":"markdown","source":["# PyTorch Introduction\n","## PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.\n","### Can be seen as substitute of NumPy with GPU capabilities\n","A Tensor is just a more generic term than matrix or vector.\n","PyTorch Tensors There appear to be 4 major types of tensors in PyTorch: Byte, Float, Double, and Long tensors. Each tensor type corresponds to the type of number (and more importantly the size/preision of the number) contained in each place of the matrix.\n","\n","\n","## NumPy:\n","NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more.\n","* For Numpy docs: Refer the following https://docs.scipy.org/doc/numpy-1.13.0/reference/"]},{"metadata":{"id":"Lz4_VbC1krvR","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2ojijhMzkrvV","colab_type":"text"},"cell_type":"markdown","source":["## A torch.Tensor is a multi-dimensional matrix containing elements of a single data type.\n","A very similar package for `numpy.ndarray`. It basically supports almost every major computation of numpy\n","\n","### Lets dive in and see some basic pythonic operations"]},{"metadata":{"id":"ovoLTvhFkrvX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"b8596822-7f2a-4dd9-fcaa-342bb99f4506","executionInfo":{"status":"ok","timestamp":1538715628037,"user_tz":-330,"elapsed":1197,"user":{"displayName":"Raj Kumar Maurya","photoUrl":"","userId":"13774242136181979947"}}},"cell_type":"code","source":["x = torch.zeros(2,3)\n","x"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n"," 0  0  0\n"," 0  0  0\n","[torch.FloatTensor of size 2x3]"]},"metadata":{"tags":[]},"execution_count":2}]},{"metadata":{"id":"Q5WvA-cVkrva","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"042cc01b-7ff7-403e-ef9f-db852578d88c","executionInfo":{"status":"ok","timestamp":1538715632523,"user_tz":-330,"elapsed":1141,"user":{"displayName":"Raj Kumar Maurya","photoUrl":"","userId":"13774242136181979947"}}},"cell_type":"code","source":["x = torch.ones(2,3)\n","x"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n"," 1  1  1\n"," 1  1  1\n","[torch.FloatTensor of size 2x3]"]},"metadata":{"tags":[]},"execution_count":3}]},{"metadata":{"id":"kijyVShXkrvd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"outputId":"dc466fd7-9783-4c35-d369-6d025ed0c4e7","executionInfo":{"status":"ok","timestamp":1538715663122,"user_tz":-330,"elapsed":1105,"user":{"displayName":"Raj Kumar Maurya","photoUrl":"","userId":"13774242136181979947"}}},"cell_type":"code","source":["# torch.arange(start,end,step=1) -> [start,end) with step\n","x = torch.arange(0,3,step=0.5)\n","x\n"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n"," 0.0000\n"," 0.5000\n"," 1.0000\n"," 1.5000\n"," 2.0000\n"," 2.5000\n","[torch.FloatTensor of size 6]"]},"metadata":{"tags":[]},"execution_count":4}]},{"metadata":{"id":"P9evyVZQkrvf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"outputId":"3850c940-07f5-4328-c3ee-effe95281f45","executionInfo":{"status":"ok","timestamp":1538716039555,"user_tz":-330,"elapsed":1273,"user":{"displayName":"Raj Kumar Maurya","photoUrl":"","userId":"13774242136181979947"}}},"cell_type":"code","source":["# torch.FloatTensor(size or list)\n","x = torch.FloatTensor(2,3)\n","x\n"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","1.00000e-36 *\n","  1.6396  0.0000  0.0000\n","  0.0000  0.0000  0.0000\n","[torch.FloatTensor of size 2x3]"]},"metadata":{"tags":[]},"execution_count":6}]},{"metadata":{"id":"-Yh87qOqkrvi","colab_type":"text"},"cell_type":"markdown","source":["## Convert NumPy to PyTorch and vice-versa\n","With almost no computation cost, you can convert PyTorch tensor to NumPy array and any change in the converted NumPy array will reflect on the original PyTorch tensor"]},{"metadata":{"id":"Ir3NLFm4krvj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"2dbe2972-b728-4147-df7b-cdfd31648183","executionInfo":{"status":"ok","timestamp":1538716091769,"user_tz":-330,"elapsed":1117,"user":{"displayName":"Raj Kumar Maurya","photoUrl":"","userId":"13774242136181979947"}}},"cell_type":"code","source":["import numpy as np\n","\n","# torch.from_numpy(ndarray) -> tensor\n","\n","x1 = np.ndarray(shape=(2,3), dtype=int,buffer=np.array([1,2,3,4,5,6]))\n","x2 = torch.from_numpy(x1)\n","\n","x2"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n"," 1  2  3\n"," 4  5  6\n","[torch.LongTensor of size 2x3]"]},"metadata":{"tags":[]},"execution_count":7}]},{"metadata":{"id":"ublzIl9kkrvm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"386e0f4e-0cd6-46f4-ed19-c19161af4e61","executionInfo":{"status":"ok","timestamp":1538716157512,"user_tz":-330,"elapsed":1113,"user":{"displayName":"Raj Kumar Maurya","photoUrl":"","userId":"13774242136181979947"}}},"cell_type":"code","source":["# tensor.numpy() -> ndarray\n","x3 = x2.numpy()\n","x3\n","\n","# Defining a numpy array and converting it to a Torch tensor\n","# a = np.ndarray(shape=(2,3), dtype=float)\n","# a = torch.from_numpy(a)\n","# a"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1, 2, 3],\n","       [4, 5, 6]])"]},"metadata":{"tags":[]},"execution_count":8}]},{"metadata":{"id":"ZAXIpkYbkrvn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"dbc34a10-e832-4a71-f273-324cf75425ef","executionInfo":{"status":"ok","timestamp":1538716162130,"user_tz":-330,"elapsed":1362,"user":{"displayName":"Raj Kumar Maurya","photoUrl":"","userId":"13774242136181979947"}}},"cell_type":"code","source":["x = torch.FloatTensor([[1,2,3],[4,5,6]])\n","x"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n"," 1  2  3\n"," 4  5  6\n","[torch.FloatTensor of size 2x3]"]},"metadata":{"tags":[]},"execution_count":9}]},{"metadata":{"id":"mVtVhj8qkrvr","colab_type":"code","colab":{}},"cell_type":"code","source":["# x_gpu = x.cuda()\n","# x_gpu"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DCevhE4qkrvv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"02b95792-cfd2-44ff-c1a7-012abb8906de","executionInfo":{"status":"ok","timestamp":1538716393641,"user_tz":-330,"elapsed":1124,"user":{"displayName":"Raj Kumar Maurya","photoUrl":"","userId":"13774242136181979947"}}},"cell_type":"code","source":["# tensor.size() -> indexing also possible\n","\n","x = torch.FloatTensor(10,12,3,3)\n","\n","x.size()[:]\n","\n","# x.size()[:2]\n","# x.size()"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10, 12, 3, 3])"]},"metadata":{"tags":[]},"execution_count":12}]},{"metadata":{"id":"7anGQyYIkrvx","colab_type":"text"},"cell_type":"markdown","source":["## The contents of a tensor can be accessed and modified using Pythonâ€™s indexing and slicing notation:"]},{"metadata":{"id":"kLh3fK9mkrvx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"outputId":"c93c61d5-79ce-4780-cb80-a7c4af0ba05a","executionInfo":{"status":"ok","timestamp":1538716399530,"user_tz":-330,"elapsed":1110,"user":{"displayName":"Raj Kumar Maurya","photoUrl":"","userId":"13774242136181979947"}}},"cell_type":"code","source":["# torch.index_select(input, dim, index)\n","\n","x = torch.rand(4,3)\n","out = torch.index_select(x,0,torch.LongTensor([0,3]))\n","\n","x,out\n"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(\n","  0.4400  0.8690  0.1604\n","  0.2888  0.4016  0.6627\n","  0.0776  0.9876  0.1926\n","  0.4662  0.3892  0.5683\n"," [torch.FloatTensor of size 4x3], \n","  0.4400  0.8690  0.1604\n","  0.4662  0.3892  0.5683\n"," [torch.FloatTensor of size 2x3])"]},"metadata":{"tags":[]},"execution_count":13}]},{"metadata":{"id":"K8TXmrVykrvz","colab_type":"text"},"cell_type":"markdown","source":["### Pythonic Indexing"]},{"metadata":{"id":"wRdKgEv6krv0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":243},"outputId":"f029ee07-35ba-4464-8e00-7a82798a7172","executionInfo":{"status":"ok","timestamp":1538716419813,"user_tz":-330,"elapsed":1109,"user":{"displayName":"Raj Kumar Maurya","photoUrl":"","userId":"13774242136181979947"}}},"cell_type":"code","source":["# pythonic indexing also works\n","\n","x[:,0],x[0,:],x[0:2,0:2]\n","\n","# name = 'abhishek'\n","# name = name[0:2]\n","# name"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(\n","  0.4400\n","  0.2888\n","  0.0776\n","  0.4662\n"," [torch.FloatTensor of size 4], \n","  0.4400\n","  0.8690\n","  0.1604\n"," [torch.FloatTensor of size 3], \n","  0.4400  0.8690\n","  0.2888  0.4016\n"," [torch.FloatTensor of size 2x2])"]},"metadata":{"tags":[]},"execution_count":14}]},{"metadata":{"id":"dTKGGX0akrv3","colab_type":"text"},"cell_type":"markdown","source":["### Torch masking"]},{"metadata":{"id":"jM4z5bbOkrv4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":191},"outputId":"76b12fbd-34ea-44c7-bbfd-6806a938c42b","executionInfo":{"status":"ok","timestamp":1538716434518,"user_tz":-330,"elapsed":1107,"user":{"displayName":"Raj Kumar Maurya","photoUrl":"","userId":"13774242136181979947"}}},"cell_type":"code","source":["# torch.masked_select(input, mask)\n","\n","x = torch.randn(2,3)\n","mask = torch.ByteTensor([[0,0,1],[0,1,0]])\n","out = torch.masked_select(x,mask)\n","\n","x, mask, out"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(\n","  0.2176 -0.4504  1.0259\n"," -0.2393  0.7895 -0.3230\n"," [torch.FloatTensor of size 2x3], \n","  0  0  1\n","  0  1  0\n"," [torch.ByteTensor of size 2x3], \n","  1.0259\n","  0.7895\n"," [torch.FloatTensor of size 2])"]},"metadata":{"tags":[]},"execution_count":15}]},{"metadata":{"id":"FiECLmDRkrv7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":277},"outputId":"25288b60-dcc2-4f56-8450-eed487162c57","executionInfo":{"status":"ok","timestamp":1538716453666,"user_tz":-330,"elapsed":1420,"user":{"displayName":"Raj Kumar Maurya","photoUrl":"","userId":"13774242136181979947"}}},"cell_type":"code","source":["# torch.cat(seq, dim=0) -> concatenate tensor along dim\n","\n","x = torch.FloatTensor([[1,2,3],[4,5,6]])\n","y = torch.FloatTensor([[-1,-2,-3],[-4,-5,-6]])\n","z1 = torch.cat([x,y],dim=0)\n","z2 = torch.cat([x,y],dim=1)\n","\n","x,y,z1,z2\n"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(\n","  1  2  3\n","  4  5  6\n"," [torch.FloatTensor of size 2x3], \n"," -1 -2 -3\n"," -4 -5 -6\n"," [torch.FloatTensor of size 2x3], \n","  1  2  3\n","  4  5  6\n"," -1 -2 -3\n"," -4 -5 -6\n"," [torch.FloatTensor of size 4x3], \n","  1  2  3 -1 -2 -3\n","  4  5  6 -4 -5 -6\n"," [torch.FloatTensor of size 2x6])"]},"metadata":{"tags":[]},"execution_count":16}]},{"metadata":{"id":"4fdc0HKLkrv9","colab_type":"text"},"cell_type":"markdown","source":["## Math Functions\n","\n","### Torch provides MATLAB-like functions for manipulating Tensor objects. \n","\n","`torch.add(tensor, value)`\n","Add the given value to all elements in the `Tensor`.\n","\n","\n","`y = torch.add(x, value)` returns a new `Tensor`.\n","\n","`x:add(value)` add `value` to all elements in place."]},{"metadata":{"id":"y7LYV4vMkrv-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":295},"outputId":"a977a9d3-2ca2-499f-a45f-94806b1766c9","executionInfo":{"status":"ok","timestamp":1538716533468,"user_tz":-330,"elapsed":1315,"user":{"displayName":"Raj Kumar Maurya","photoUrl":"","userId":"13774242136181979947"}}},"cell_type":"code","source":["# torch.add()\n","\n","x1 = torch.FloatTensor([[1,2,3],[4,5,6]])\n","x2 = torch.FloatTensor([[1,2,3],[4,5,6]])\n","add = torch.add(x1,x2)\n","\n","x1,x2,add,x1+x2,x1-x2\n"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(\n","  1  2  3\n","  4  5  6\n"," [torch.FloatTensor of size 2x3], \n","  1  2  3\n","  4  5  6\n"," [torch.FloatTensor of size 2x3], \n","   2   4   6\n","   8  10  12\n"," [torch.FloatTensor of size 2x3], \n","   2   4   6\n","   8  10  12\n"," [torch.FloatTensor of size 2x3], \n","  0  0  0\n","  0  0  0\n"," [torch.FloatTensor of size 2x3])"]},"metadata":{"tags":[]},"execution_count":17}]},{"metadata":{"id":"wFNmiTLbkrwB","colab_type":"text"},"cell_type":"markdown","source":["### Matrix matrix product of `mat1` and `mat2`. \n","If mat1 is a n Ã— m matrix, mat2 a m Ã— p matrix, res must be a n Ã— p matrix.\n","\n","`torch.mm(x, y)` puts the result in a new Tensor.\n","\n","`torch.mm(M, x, y)` puts the result in M.\n","\n","`M:mm(x, y)` puts the result in M."]},{"metadata":{"id":"GESoLXitkrwB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"outputId":"2d6e8f68-c2ec-4669-b7e7-0dba5acbac19","executionInfo":{"status":"ok","timestamp":1538716575411,"user_tz":-330,"elapsed":1086,"user":{"displayName":"Raj Kumar Maurya","photoUrl":"","userId":"13774242136181979947"}}},"cell_type":"code","source":["# torch.mm(mat1, mat2) -> matrix multiplication\n","\n","x1 = torch.FloatTensor(3,4)\n","x2 = torch.FloatTensor(4,5)\n","\n","torch.mm(x1,x2)"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n"," 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00         nan\n","-1.4584e-23  2.5023e+36         inf  2.6521e+31         nan\n"," 0.0000e+00 -2.0736e-28  0.0000e+00  0.0000e+00         nan\n","[torch.FloatTensor of size 3x5]"]},"metadata":{"tags":[]},"execution_count":18}]},{"metadata":{"id":"Klvvjg9vkrwD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":208},"outputId":"194b1125-3d8d-4e48-876c-852b5519b7ea","executionInfo":{"status":"ok","timestamp":1538720691575,"user_tz":-330,"elapsed":1002,"user":{"displayName":"Raj Kumar Maurya","photoUrl":"","userId":"13774242136181979947"}}},"cell_type":"code","source":["# torch.eig(a,eigenvectors=False) -> eigen_value, eigen_vector\n","\n","import torch\n","\n","x1 = torch.FloatTensor(4,4)\n","\n","x1 = torch.eig(x1,True)\n","\n","x1"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(\n","  0.0000e+00  0.0000e+00\n","  1.0141e+31  0.0000e+00\n","  0.0000e+00  0.0000e+00\n","  0.0000e+00  0.0000e+00\n"," [torch.FloatTensor of size 4x2], \n"," -1.0000 -0.2425  0.0000  0.0000\n","  0.0000  0.0000  1.0000 -1.0000\n","  0.0000 -0.9701  0.0000  0.0000\n","  0.0000  0.0000  0.0000  0.0000\n"," [torch.FloatTensor of size 4x4])"]},"metadata":{"tags":[]},"execution_count":1}]},{"metadata":{"id":"BZ9OpUFzkrwF","colab_type":"text"},"cell_type":"markdown","source":["## PyTorch Autograd"]},{"metadata":{"id":"el42baSlkrwF","colab_type":"code","colab":{}},"cell_type":"code","source":["from torch.autograd import Variable"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DZYkc-eckrwI","colab_type":"text"},"cell_type":"markdown","source":["### Autograd is now a core torch package for automatic differentiation. It uses a tape based system for automatic differentiation.\n","\n","In autograd, there is a Variable class, which is a very thin wrapper around a Tensor. \n","You can access the raw tensor through the `.data attribute`, and after computing the backward pass, a gradient w.r.t. this variable is accumulated into `.grad attribute`.\n","\n","#### We wrap our PyTorch Tensors in Variable objects; a Variable represents a node in a computational graph. If x is a Variable then `x.data` is a Tensor, and `x.grad` is another Variable holding the gradient of x with respect to some scalar value."]},{"metadata":{"id":"cVTndCuIkrwI","colab_type":"code","colab":{}},"cell_type":"code","source":["dtype = torch.FloatTensor\n","# dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n","\n","# N is batch size; D_in is input dimension;\n","# H is hidden dimension; D_out is output dimension.\n","N, D_in, H, D_out = 64, 1000, 100, 10"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TQKKQhTQkrwL","colab_type":"text"},"cell_type":"markdown","source":["Create random Tensors to hold input and outputs, and wrap them in Variables.\n","Setting `requires_grad=False` indicates that we do not need to compute gradients with respect to these Variables during the backward pass.\n"]},{"metadata":{"id":"ECdk2T2lkrwM","colab_type":"code","colab":{}},"cell_type":"code","source":["x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\n","y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"h1T1zhMHkrwO","colab_type":"text"},"cell_type":"markdown","source":["#### Create random Tensors for weights, and wrap them in Variables.\n","#### Setting requires_grad=True indicates that we want to compute gradients with respect to these Variables during the backward pass."]},{"metadata":{"id":"NDbaLDgHkrwP","colab_type":"code","colab":{}},"cell_type":"code","source":["w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=True)\n","w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4ykV-shZkrwR","colab_type":"text"},"cell_type":"markdown","source":["### Forward pass: \n","compute predicted y using operations on Variables\n","\n","\n","### Use autograd:\n","to compute the backward pass. This call will compute the gradient of loss with respect to all Variables with requires_grad=True.\n","\n","After this call w1.grad and w2.grad will be Variables holding the gradient of the loss with respect to w1 and w2 respectively.\n","\n","\n","### Update weights:\n","using gradient descent; w1.data and w2.data are Tensors, w1.grad and w2.grad are Variables and w1.grad.data and w2.grad.data are Tensors.\n","\n","#### Manually zero the gradients after running the backward pass"]},{"metadata":{"id":"1RceoWF4krwR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":8684},"outputId":"f5dc8da7-01e2-42cc-9ea2-03ba9184f3f0","executionInfo":{"status":"ok","timestamp":1538720907995,"user_tz":-330,"elapsed":2143,"user":{"displayName":"Raj Kumar Maurya","photoUrl":"","userId":"13774242136181979947"}}},"cell_type":"code","source":["learning_rate = 1e-6\n","for t in range(500):\n","  y_pred = x.mm(w1).clamp(min=0).mm(w2)\n","  \n","  # Compute and print loss using operations on Variables.\n","  # Now loss is a Variable of shape (1,) and loss.data is a Tensor of shape\n","  # (1,); loss.data[0] is a scalar value holding the loss.\n","  loss = (y_pred - y).pow(2).sum()\n","  print(t, loss.data[0])\n","\n","\n","  loss.backward()\n","\n","  w1.data -= learning_rate * w1.grad.data\n","  w2.data -= learning_rate * w2.grad.data\n","\n","  # Manually zero the gradients after running the backward pass\n","  w1.grad.data.zero_()\n","  w2.grad.data.zero_()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["0 50867244.0\n","1 57149276.0\n","2 59574760.0\n","3 43956764.0\n","4 21238236.0\n","5 7760386.0\n","6 3358652.75\n","7 2050725.125\n","8 1531127.125\n","9 1229176.875\n","10 1013300.1875\n","11 846491.3125\n","12 713454.875\n","13 605582.8125\n","14 517306.40625\n","15 444503.15625\n","16 383980.75\n","17 333238.09375\n","18 290512.15625\n","19 254387.984375\n","20 223613.859375\n","21 197239.8125\n","22 174497.671875\n","23 154867.0625\n","24 137839.84375\n","25 123038.9296875\n","26 110122.6796875\n","27 98811.515625\n","28 88866.5546875\n","29 80097.0390625\n","30 72335.2265625\n","31 65446.3359375\n","32 59321.10546875\n","33 53864.28125\n","34 48992.8125\n","35 44632.23828125\n","36 40721.296875\n","37 37206.52734375\n","38 34041.65234375\n","39 31188.408203125\n","40 28610.720703125\n","41 26275.166015625\n","42 24158.037109375\n","43 22236.06640625\n","44 20487.91796875\n","45 18896.255859375\n","46 17446.169921875\n","47 16122.447265625\n","48 14914.0849609375\n","49 13808.0078125\n","50 12794.35546875\n","51 11864.44140625\n","52 11010.41796875\n","53 10224.904296875\n","54 9502.67578125\n","55 8837.1943359375\n","56 8223.6123046875\n","57 7657.494140625\n","58 7134.43212890625\n","59 6651.041015625\n","60 6203.84716796875\n","61 5789.8583984375\n","62 5406.27197265625\n","63 5050.9189453125\n","64 4721.07080078125\n","65 4414.75537109375\n","66 4130.18994140625\n","67 3865.6494140625\n","68 3619.566650390625\n","69 3390.551025390625\n","70 3177.318603515625\n","71 2978.57080078125\n","72 2793.311279296875\n","73 2620.54736328125\n","74 2459.3251953125\n","75 2308.82080078125\n","76 2168.3486328125\n","77 2037.056884765625\n","78 1914.3443603515625\n","79 1799.5841064453125\n","80 1692.213134765625\n","81 1591.697265625\n","82 1497.607177734375\n","83 1409.52099609375\n","84 1326.9825439453125\n","85 1249.6143798828125\n","86 1177.054931640625\n","87 1108.982666015625\n","88 1045.094970703125\n","89 985.1237182617188\n","90 928.8182373046875\n","91 875.9340209960938\n","92 826.2298583984375\n","93 779.5127563476562\n","94 735.5848388671875\n","95 694.2708129882812\n","96 655.4208374023438\n","97 618.8655395507812\n","98 584.4780883789062\n","99 552.0869140625\n","100 521.5859985351562\n","101 492.8565673828125\n","102 465.786865234375\n","103 440.2843322753906\n","104 416.2497863769531\n","105 393.5904235839844\n","106 372.23663330078125\n","107 352.08172607421875\n","108 333.072509765625\n","109 315.1402587890625\n","110 298.2134094238281\n","111 282.2413635253906\n","112 267.1634521484375\n","113 252.9305419921875\n","114 239.4861602783203\n","115 226.7847900390625\n","116 214.7881622314453\n","117 203.4458465576172\n","118 192.7315673828125\n","119 182.60443115234375\n","120 173.03126525878906\n","121 163.9850311279297\n","122 155.42494201660156\n","123 147.32806396484375\n","124 139.66893005371094\n","125 132.42465209960938\n","126 125.5716323852539\n","127 119.08357238769531\n","128 112.94551849365234\n","129 107.13426971435547\n","130 101.63419342041016\n","131 96.42473602294922\n","132 91.49363708496094\n","133 86.8194351196289\n","134 82.39540100097656\n","135 78.20369720458984\n","136 74.2308578491211\n","137 70.46768188476562\n","138 66.90342712402344\n","139 63.522621154785156\n","140 60.318824768066406\n","141 57.28183364868164\n","142 54.40256118774414\n","143 51.67351150512695\n","144 49.084495544433594\n","145 46.623905181884766\n","146 44.290245056152344\n","147 42.078407287597656\n","148 39.98051071166992\n","149 37.98941421508789\n","150 36.10206985473633\n","151 34.309635162353516\n","152 32.609378814697266\n","153 30.99594497680664\n","154 29.463512420654297\n","155 28.00993537902832\n","156 26.629581451416016\n","157 25.319425582885742\n","158 24.07521629333496\n","159 22.894685745239258\n","160 21.772825241088867\n","161 20.707469940185547\n","162 19.695175170898438\n","163 18.73423957824707\n","164 17.820919036865234\n","165 16.953266143798828\n","166 16.129114151000977\n","167 15.346159934997559\n","168 14.602453231811523\n","169 13.895554542541504\n","170 13.223204612731934\n","171 12.58444595336914\n","172 11.977374076843262\n","173 11.399749755859375\n","174 10.851263999938965\n","175 10.32947063446045\n","176 9.833632469177246\n","177 9.361985206604004\n","178 8.913370132446289\n","179 8.487130165100098\n","180 8.081474304199219\n","181 7.695347309112549\n","182 7.328315258026123\n","183 6.978885173797607\n","184 6.647017002105713\n","185 6.330901622772217\n","186 6.029994010925293\n","187 5.7440185546875\n","188 5.471752166748047\n","189 5.212863445281982\n","190 4.965813636779785\n","191 4.7316131591796875\n","192 4.508214950561523\n","193 4.295625686645508\n","194 4.093341827392578\n","195 3.900557518005371\n","196 3.717197895050049\n","197 3.5425097942352295\n","198 3.376391887664795\n","199 3.2180190086364746\n","200 3.0673041343688965\n","201 2.9239144325256348\n","202 2.7870266437530518\n","203 2.6569061279296875\n","204 2.532896041870117\n","205 2.414954423904419\n","206 2.3022420406341553\n","207 2.1950879096984863\n","208 2.0931639671325684\n","209 1.9957308769226074\n","210 1.902992844581604\n","211 1.814772129058838\n","212 1.730555534362793\n","213 1.6502734422683716\n","214 1.5738368034362793\n","215 1.5012012720108032\n","216 1.4317853450775146\n","217 1.3656686544418335\n","218 1.3025527000427246\n","219 1.2424882650375366\n","220 1.1853290796279907\n","221 1.1306363344192505\n","222 1.0786614418029785\n","223 1.0289394855499268\n","224 0.9816898107528687\n","225 0.9366855621337891\n","226 0.8937274217605591\n","227 0.8527358174324036\n","228 0.8136752247810364\n","229 0.7764616012573242\n","230 0.7408637404441833\n","231 0.7069893479347229\n","232 0.6746407151222229\n","233 0.6439056396484375\n","234 0.6145225763320923\n","235 0.5865550637245178\n","236 0.5598084330558777\n","237 0.5343721508979797\n","238 0.5100507736206055\n","239 0.486844539642334\n","240 0.46469858288764954\n","241 0.44367092847824097\n","242 0.42351487278938293\n","243 0.4043123126029968\n","244 0.3859575092792511\n","245 0.3685450553894043\n","246 0.3518178164958954\n","247 0.33593544363975525\n","248 0.3207666873931885\n","249 0.30625250935554504\n","250 0.29242727160453796\n","251 0.2792406678199768\n","252 0.266626238822937\n","253 0.25462061166763306\n","254 0.24315588176250458\n","255 0.23221318423748016\n","256 0.22178688645362854\n","257 0.2118099182844162\n","258 0.2022867649793625\n","259 0.19325357675552368\n","260 0.18458011746406555\n","261 0.17625710368156433\n","262 0.16839101910591125\n","263 0.16082853078842163\n","264 0.15362584590911865\n","265 0.14674542844295502\n","266 0.1401844471693039\n","267 0.133916437625885\n","268 0.1279325783252716\n","269 0.12223535031080246\n","270 0.11674709618091583\n","271 0.11154413223266602\n","272 0.10660166293382645\n","273 0.10186443477869034\n","274 0.09731409698724747\n","275 0.09298625588417053\n","276 0.08883626759052277\n","277 0.08488073945045471\n","278 0.0811069905757904\n","279 0.0775129646062851\n","280 0.07405655831098557\n","281 0.07079290598630905\n","282 0.067646823823452\n","283 0.06465093046426773\n","284 0.06178371608257294\n","285 0.05905191972851753\n","286 0.05644863098859787\n","287 0.05396037548780441\n","288 0.051593951880931854\n","289 0.049301695078611374\n","290 0.04712919890880585\n","291 0.04505430534482002\n","292 0.04305834323167801\n","293 0.04115474969148636\n","294 0.03935263678431511\n","295 0.03761427849531174\n","296 0.03595893085002899\n","297 0.03438711538910866\n","298 0.03288784250617027\n","299 0.031435541808605194\n","300 0.030063867568969727\n","301 0.028743067756295204\n","302 0.027486663311719894\n","303 0.026294730603694916\n","304 0.025150269269943237\n","305 0.024048753082752228\n","306 0.02300134114921093\n","307 0.021999021992087364\n","308 0.021033570170402527\n","309 0.020126506686210632\n","310 0.019265733659267426\n","311 0.01843137852847576\n","312 0.017627941444516182\n","313 0.016860634088516235\n","314 0.01613273099064827\n","315 0.015439347364008427\n","316 0.01477469690144062\n","317 0.014131980948150158\n","318 0.013527494855225086\n","319 0.012949911877512932\n","320 0.01240374706685543\n","321 0.011869428679347038\n","322 0.011361480690538883\n","323 0.010883710347115993\n","324 0.010423416271805763\n","325 0.009979670867323875\n","326 0.009551131166517735\n","327 0.009153811261057854\n","328 0.008765118196606636\n","329 0.008396226912736893\n","330 0.008045559749007225\n","331 0.00770462304353714\n","332 0.007386756129562855\n","333 0.0070795537903904915\n","334 0.006778821349143982\n","335 0.00650312565267086\n","336 0.006230416242033243\n","337 0.0059778932482004166\n","338 0.005732047837227583\n","339 0.005501576233655214\n","340 0.005272123962640762\n","341 0.005054525099694729\n","342 0.004843663889914751\n","343 0.004647006746381521\n","344 0.0044598826207220554\n","345 0.0042801215313375\n","346 0.00410804245620966\n","347 0.00393997086212039\n","348 0.003784752218052745\n","349 0.0036349159199744463\n","350 0.0034892833791673183\n","351 0.0033486078027635813\n","352 0.0032194405794143677\n","353 0.0030920456629246473\n","354 0.002970912493765354\n","355 0.00285335723310709\n","356 0.0027458560653030872\n","357 0.002634734148159623\n","358 0.0025354158133268356\n","359 0.002438754541799426\n","360 0.0023453233297914267\n","361 0.0022592570167034864\n","362 0.002173120155930519\n","363 0.0020882654935121536\n","364 0.0020128104370087385\n","365 0.001939990557730198\n","366 0.0018696467159315944\n","367 0.0018004546873271465\n","368 0.0017342237988486886\n","369 0.0016695578815415502\n","370 0.0016124960966408253\n","371 0.001552517875097692\n","372 0.0014973515644669533\n","373 0.001444609952159226\n","374 0.0013947943225502968\n","375 0.0013447265373542905\n","376 0.0012986899819225073\n","377 0.0012560205068439245\n","378 0.001212787115946412\n","379 0.001168867340311408\n","380 0.001130316057242453\n","381 0.0010940302163362503\n","382 0.0010557611240074039\n","383 0.0010217850795015693\n","384 0.0009878891287371516\n","385 0.0009559207246638834\n","386 0.0009254219476133585\n","387 0.0008958653779700398\n","388 0.0008674411801621318\n","389 0.0008395549957640469\n","390 0.0008122927392832935\n","391 0.0007882285863161087\n","392 0.0007643476128578186\n","393 0.0007415659492835402\n","394 0.000718149880412966\n","395 0.0006972355186007917\n","396 0.0006761891418136656\n","397 0.0006552256527356803\n","398 0.0006352185737341642\n","399 0.0006161050987429917\n","400 0.0005974383093416691\n","401 0.0005797437625005841\n","402 0.0005629801307804883\n","403 0.0005464669084176421\n","404 0.0005318645853549242\n","405 0.0005168992793187499\n","406 0.00050362414913252\n","407 0.0004875735903624445\n","408 0.0004742262826766819\n","409 0.00046129815746098757\n","410 0.00044764106860384345\n","411 0.0004350002855062485\n","412 0.0004246061434969306\n","413 0.00041282494203187525\n","414 0.00040209211874753237\n","415 0.000391021283576265\n","416 0.0003805904998444021\n","417 0.00037130783312022686\n","418 0.00036132289096713066\n","419 0.000351964496076107\n","420 0.0003423738235142082\n","421 0.0003335897345095873\n","422 0.0003260691009927541\n","423 0.0003170191776007414\n","424 0.00030898500699549913\n","425 0.00030162135954014957\n","426 0.00029422470834106207\n","427 0.00028711435152217746\n","428 0.00027977535501122475\n","429 0.0002733551082201302\n","430 0.0002670460962690413\n","431 0.0002604851615615189\n","432 0.00025453517446294427\n","433 0.000248161144554615\n","434 0.00024261757789645344\n","435 0.00023705611238256097\n","436 0.00023103496641851962\n","437 0.00022606956190429628\n","438 0.00022106687538325787\n","439 0.0002157819108106196\n","440 0.00021137147268746048\n","441 0.000206193930353038\n","442 0.0002011480974033475\n","443 0.00019719214469660074\n","444 0.00019332004012539983\n","445 0.00018917812849394977\n","446 0.00018528751388657838\n","447 0.00018101207388099283\n","448 0.00017743413627613336\n","449 0.00017390177526976913\n","450 0.0001701399451121688\n","451 0.00016677295207045972\n","452 0.0001631834456929937\n","453 0.00016054321895353496\n","454 0.00015649848501197994\n","455 0.00015335834177676588\n","456 0.00015056502888910472\n","457 0.00014721776824444532\n","458 0.00014438439393416047\n","459 0.00014179515710566193\n","460 0.00013897463213652372\n","461 0.00013635054347105324\n","462 0.00013391788525041193\n","463 0.00013182676048018038\n","464 0.00012908190547022969\n","465 0.0001261954166693613\n","466 0.0001236622774740681\n","467 0.00012154286378063262\n","468 0.00011935725342482328\n","469 0.00011708366218954325\n","470 0.00011555592936929315\n","471 0.00011320172052364796\n","472 0.00011139972048113123\n","473 0.00010909357661148533\n","474 0.00010730731446528807\n","475 0.00010522668162593618\n","476 0.00010361638123868033\n","477 0.00010150922753382474\n","478 9.994010906666517e-05\n","479 9.81786142801866e-05\n","480 9.65642902883701e-05\n","481 9.475602564634755e-05\n","482 9.308535663876683e-05\n","483 9.144980867858976e-05\n","484 8.971440547611564e-05\n","485 8.866997086443007e-05\n","486 8.718638855498284e-05\n","487 8.582641748944297e-05\n","488 8.451130997855216e-05\n","489 8.32576333777979e-05\n","490 8.199513831641525e-05\n","491 8.008705481188372e-05\n","492 7.885068043833598e-05\n","493 7.764674228383228e-05\n","494 7.646913581993431e-05\n","495 7.528493733843789e-05\n","496 7.397646550089121e-05\n","497 7.251845818245783e-05\n","498 7.165001443354413e-05\n","499 7.061162614263594e-05\n"],"name":"stdout"}]},{"metadata":{"id":"FMayOegjkrwT","colab_type":"text"},"cell_type":"markdown","source":["# PyTorch NN module\n","\n","The nn package defines a set of Modules, which you can think of as a neural network layer that has produces output from input and may have some trainable weights.\n","\n"]},{"metadata":{"id":"1dyAuvafkrwU","colab_type":"code","colab":{}},"cell_type":"code","source":["N, D_in, H, D_out = 64, 1000, 100, 10"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zzUqwblOkrwW","colab_type":"text"},"cell_type":"markdown","source":["### Create random Tensors to hold inputs and outputs, and wrap them in Variables.\n"]},{"metadata":{"id":"d2--gFTAkrwX","colab_type":"code","colab":{}},"cell_type":"code","source":["x = Variable(torch.randn(N, D_in))\n","y = Variable(torch.randn(N, D_out), requires_grad=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Km-JpuqXkrwb","colab_type":"text"},"cell_type":"markdown","source":["### Use the nn package to define our model as a sequence of layers.\n","nn.Sequential is a Module which contains other Modules, and applies them in sequence to produce its output. \n","Each Linear Module computes output from input using a linear function, and holds internal Variables for its weight and bias.\n"]},{"metadata":{"id":"gtbSpp-Vkrwc","colab_type":"code","colab":{}},"cell_type":"code","source":["model = torch.nn.Sequential(\n","          torch.nn.Linear(D_in, H),\n","          torch.nn.ReLU(),\n","          torch.nn.Linear(H, D_out),\n","        )"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vQMfaayQkrwf","colab_type":"text"},"cell_type":"markdown","source":["### Loss Function:\n","The nn package also contains definitions of popular loss functions; in this case we will use Mean Squared Error (MSE) as our loss function.\n","We'll also initialise the learning rate\n"]},{"metadata":{"id":"ZV31Ho-Skrwg","colab_type":"code","colab":{}},"cell_type":"code","source":["loss_fn = torch.nn.MSELoss(size_average=False)\n","\n","learning_rate = 1e-4"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GIasQ128krwm","colab_type":"text"},"cell_type":"markdown","source":["## Optimization:\n","Use the optim package to define an Optimizer that will update the weights of the model for us. \n","\n","Here we will use Adam; the optim package contains many otheroptimization algoriths. \n","The first argument to the Adam constructor tells the optimizer which Variables it should update."]},{"metadata":{"id":"ARJwkooPkrwm","colab_type":"code","colab":{}},"cell_type":"code","source":["optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0TAtwlAzkrwp","colab_type":"text"},"cell_type":"markdown","source":["### Forward pass: \n","compute predicted y by passing x to the model. Module objects override the __call__ operator so you can call them like functions. \n","When doing so you pass a Variable of input data to the Module and it produces a Variable of output data.\n","\n","### Compute Loss\n","\n","### Zero the gradients before running the backward pass.\n","\n","### Backward pass: \n","compute gradient of the loss with respect to all the learnable parameters of the model. Internally, the parameters of each Module are stored in Variables with requires_grad=True, so this call will compute gradients for all learnable parameters in the model.\n","\n","### Update the weights using gradient descent. "]},{"metadata":{"id":"vRWi18jXkrwp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":8684},"outputId":"6c9852f4-4241-492c-d5a1-04261d8e90ec","executionInfo":{"status":"ok","timestamp":1538721238470,"user_tz":-330,"elapsed":2514,"user":{"displayName":"Raj Kumar Maurya","photoUrl":"","userId":"13774242136181979947"}}},"cell_type":"code","source":["for t in range(500):\n","  y_pred = model(x)\n","\n","  # Compute and print loss.\n","  loss = loss_fn(y_pred, y)\n","  print(t, loss.data[0])\n","  \n","  # Before the backward pass, use the optimizer object to zero all of the\n","  # gradients for the variables it will update (which are the learnable weights\n","  # of the model)\n","  optimizer.zero_grad()\n","\n","  # Backward pass: compute gradient of the loss with respect to model parameters\n","  loss.backward()\n","\n","  # Calling the step function on an Optimizer makes an update to its parameters\n","  optimizer.step()"],"execution_count":12,"outputs":[{"output_type":"stream","text":["0 667.74169921875\n","1 651.0390625\n","2 634.7952880859375\n","3 619.0105590820312\n","4 603.671875\n","5 588.7374267578125\n","6 574.2692260742188\n","7 560.21728515625\n","8 546.5621337890625\n","9 533.320556640625\n","10 520.5311279296875\n","11 508.0982666015625\n","12 496.0144348144531\n","13 484.30792236328125\n","14 472.955810546875\n","15 461.9398193359375\n","16 451.2313232421875\n","17 440.77545166015625\n","18 430.5967712402344\n","19 420.6881408691406\n","20 411.11181640625\n","21 401.78167724609375\n","22 392.6910400390625\n","23 383.80694580078125\n","24 375.1127624511719\n","25 366.630615234375\n","26 358.36395263671875\n","27 350.2791748046875\n","28 342.35455322265625\n","29 334.6158447265625\n","30 327.08428955078125\n","31 319.7550048828125\n","32 312.5661315917969\n","33 305.5269470214844\n","34 298.61810302734375\n","35 291.86126708984375\n","36 285.2330017089844\n","37 278.7474365234375\n","38 272.3753967285156\n","39 266.1274108886719\n","40 260.01263427734375\n","41 254.0327606201172\n","42 248.1527099609375\n","43 242.40402221679688\n","44 236.77120971679688\n","45 231.2448272705078\n","46 225.8367462158203\n","47 220.51226806640625\n","48 215.29273986816406\n","49 210.17596435546875\n","50 205.16172790527344\n","51 200.2603759765625\n","52 195.46136474609375\n","53 190.74879455566406\n","54 186.1147003173828\n","55 181.56369018554688\n","56 177.11036682128906\n","57 172.7523193359375\n","58 168.49365234375\n","59 164.32424926757812\n","60 160.24659729003906\n","61 156.2540283203125\n","62 152.34756469726562\n","63 148.53082275390625\n","64 144.78236389160156\n","65 141.10057067871094\n","66 137.49322509765625\n","67 133.95631408691406\n","68 130.49224853515625\n","69 127.0923080444336\n","70 123.7623291015625\n","71 120.50214385986328\n","72 117.30992126464844\n","73 114.18985748291016\n","74 111.12793731689453\n","75 108.12378692626953\n","76 105.18247985839844\n","77 102.31427764892578\n","78 99.5090560913086\n","79 96.75817108154297\n","80 94.07803344726562\n","81 91.44522857666016\n","82 88.86974334716797\n","83 86.35851287841797\n","84 83.89643859863281\n","85 81.49369812011719\n","86 79.14330291748047\n","87 76.83865356445312\n","88 74.58806610107422\n","89 72.39239501953125\n","90 70.24562072753906\n","91 68.14997863769531\n","92 66.10425567626953\n","93 64.10741424560547\n","94 62.15555191040039\n","95 60.24802017211914\n","96 58.388221740722656\n","97 56.5788459777832\n","98 54.809539794921875\n","99 53.09000778198242\n","100 51.414798736572266\n","101 49.78340530395508\n","102 48.19260025024414\n","103 46.641353607177734\n","104 45.13228225708008\n","105 43.66703796386719\n","106 42.24163818359375\n","107 40.85111999511719\n","108 39.49772262573242\n","109 38.18373107910156\n","110 36.90678787231445\n","111 35.66482162475586\n","112 34.460445404052734\n","113 33.28765869140625\n","114 32.147029876708984\n","115 31.04261016845703\n","116 29.969667434692383\n","117 28.92757225036621\n","118 27.917171478271484\n","119 26.93720245361328\n","120 25.985313415527344\n","121 25.061098098754883\n","122 24.166227340698242\n","123 23.296640396118164\n","124 22.452716827392578\n","125 21.637115478515625\n","126 20.845136642456055\n","127 20.07908821105957\n","128 19.336524963378906\n","129 18.618576049804688\n","130 17.92257308959961\n","131 17.249683380126953\n","132 16.600122451782227\n","133 15.972479820251465\n","134 15.366257667541504\n","135 14.78111457824707\n","136 14.215353965759277\n","137 13.667892456054688\n","138 13.13880729675293\n","139 12.628791809082031\n","140 12.136466979980469\n","141 11.661223411560059\n","142 11.202010154724121\n","143 10.75981616973877\n","144 10.332998275756836\n","145 9.920814514160156\n","146 9.522323608398438\n","147 9.13829517364502\n","148 8.769272804260254\n","149 8.413310050964355\n","150 8.07020378112793\n","151 7.7398576736450195\n","152 7.421821117401123\n","153 7.116025447845459\n","154 6.821534633636475\n","155 6.538246154785156\n","156 6.265963554382324\n","157 6.004271030426025\n","158 5.752856731414795\n","159 5.510809898376465\n","160 5.2786102294921875\n","161 5.055346965789795\n","162 4.84121036529541\n","163 4.635419845581055\n","164 4.4380574226379395\n","165 4.24826192855835\n","166 4.066522598266602\n","167 3.892298698425293\n","168 3.724973440170288\n","169 3.5650370121002197\n","170 3.4118878841400146\n","171 3.265040397644043\n","172 3.1244277954101562\n","173 2.98946475982666\n","174 2.8602046966552734\n","175 2.736706256866455\n","176 2.6183481216430664\n","177 2.5049893856048584\n","178 2.3963871002197266\n","179 2.2924349308013916\n","180 2.192821979522705\n","181 2.097492218017578\n","182 2.006157398223877\n","183 1.918783187866211\n","184 1.8352196216583252\n","185 1.7550770044326782\n","186 1.6784067153930664\n","187 1.6050866842269897\n","188 1.5348862409591675\n","189 1.4677060842514038\n","190 1.4034459590911865\n","191 1.341934084892273\n","192 1.2831593751907349\n","193 1.2269790172576904\n","194 1.1730360984802246\n","195 1.1215777397155762\n","196 1.0723497867584229\n","197 1.025276780128479\n","198 0.9802777767181396\n","199 0.937311589717865\n","200 0.8962327241897583\n","201 0.8569332957267761\n","202 0.819364607334137\n","203 0.7834532260894775\n","204 0.7491523623466492\n","205 0.716376781463623\n","206 0.6852359771728516\n","207 0.6554712057113647\n","208 0.6270315647125244\n","209 0.5998935699462891\n","210 0.5739399194717407\n","211 0.5491490364074707\n","212 0.5254454016685486\n","213 0.5027949810028076\n","214 0.48117178678512573\n","215 0.4604646861553192\n","216 0.4407065808773041\n","217 0.42179781198501587\n","218 0.40373608469963074\n","219 0.3864798843860626\n","220 0.36998409032821655\n","221 0.3542087972164154\n","222 0.33911705017089844\n","223 0.324693500995636\n","224 0.31089770793914795\n","225 0.29771289229393005\n","226 0.2850986421108246\n","227 0.27302560210227966\n","228 0.26148128509521484\n","229 0.2504380941390991\n","230 0.23987281322479248\n","231 0.22975945472717285\n","232 0.22008737921714783\n","233 0.21082502603530884\n","234 0.20196333527565002\n","235 0.19348427653312683\n","236 0.1853659301996231\n","237 0.17760030925273895\n","238 0.1701669692993164\n","239 0.1630570888519287\n","240 0.156254380941391\n","241 0.14973993599414825\n","242 0.14349915087223053\n","243 0.1375172734260559\n","244 0.13178841769695282\n","245 0.12630680203437805\n","246 0.12105013430118561\n","247 0.11601545661687851\n","248 0.11118951439857483\n","249 0.10656557977199554\n","250 0.10213448852300644\n","251 0.09789872169494629\n","252 0.09385235607624054\n","253 0.08997530490159988\n","254 0.08625565469264984\n","255 0.08269498497247696\n","256 0.07927924394607544\n","257 0.07600589096546173\n","258 0.07286626845598221\n","259 0.06985710561275482\n","260 0.0669684186577797\n","261 0.06420164555311203\n","262 0.06154431030154228\n","263 0.05899742990732193\n","264 0.05655462294816971\n","265 0.054209496825933456\n","266 0.05196132883429527\n","267 0.04980297386646271\n","268 0.047732874751091\n","269 0.04574637860059738\n","270 0.043840598315000534\n","271 0.04201178997755051\n","272 0.040256548672914505\n","273 0.03857221454381943\n","274 0.03695637360215187\n","275 0.03540608659386635\n","276 0.03391799330711365\n","277 0.032489579170942307\n","278 0.031119268387556076\n","279 0.02980477549135685\n","280 0.028543557971715927\n","281 0.027332719415426254\n","282 0.02617207169532776\n","283 0.02505783922970295\n","284 0.02398931421339512\n","285 0.02296416461467743\n","286 0.021980728954076767\n","287 0.021038146689534187\n","288 0.02013370208442211\n","289 0.019266003742814064\n","290 0.018434083089232445\n","291 0.017636481672525406\n","292 0.016872268170118332\n","293 0.0161387100815773\n","294 0.015435617417097092\n","295 0.014762874692678452\n","296 0.014119167812168598\n","297 0.013502411544322968\n","298 0.012910839170217514\n","299 0.012344341725111008\n","300 0.011801548302173615\n","301 0.01128152571618557\n","302 0.010783301666378975\n","303 0.01030605286359787\n","304 0.009848902933299541\n","305 0.00941112544387579\n","306 0.008991803973913193\n","307 0.008590307086706161\n","308 0.008205931633710861\n","309 0.007837830111384392\n","310 0.007485491223633289\n","311 0.0071481987833976746\n","312 0.006825389340519905\n","313 0.006516357883810997\n","314 0.0062208157032728195\n","315 0.005937913432717323\n","316 0.005667229648679495\n","317 0.005408333148807287\n","318 0.005160735920071602\n","319 0.004923893604427576\n","320 0.004697380121797323\n","321 0.004480757750570774\n","322 0.004273637663573027\n","323 0.0040756468661129475\n","324 0.0038863453082740307\n","325 0.0037056885194033384\n","326 0.0035325628705322742\n","327 0.0033673830330371857\n","328 0.003209517104551196\n","329 0.0030586805660277605\n","330 0.0029146107845008373\n","331 0.0027770192828029394\n","332 0.00264556217007339\n","333 0.002520033624023199\n","334 0.0024002199061214924\n","335 0.0022858181037008762\n","336 0.002176591195166111\n","337 0.002072358038276434\n","338 0.001972856465727091\n","339 0.0018778995145112276\n","340 0.0017873280448839068\n","341 0.0017008958384394646\n","342 0.0016184572596102953\n","343 0.0015398013638332486\n","344 0.0014647961361333728\n","345 0.0013932703295722604\n","346 0.0013250811025500298\n","347 0.001260067569091916\n","348 0.00119809212628752\n","349 0.0011390368454158306\n","350 0.0010827300138771534\n","351 0.0010290968930348754\n","352 0.0009779988322407007\n","353 0.0009293268667533994\n","354 0.0008829459547996521\n","355 0.0008387924171984196\n","356 0.0007967461715452373\n","357 0.000756703841034323\n","358 0.0007185837021097541\n","359 0.0006822991999797523\n","360 0.0006477621500380337\n","361 0.0006148823304101825\n","362 0.0005836034542880952\n","363 0.0005538538680411875\n","364 0.0005255505675449967\n","365 0.000498649722430855\n","366 0.0004730238870251924\n","367 0.0004486760008148849\n","368 0.00042552215745672584\n","369 0.00040351474308408797\n","370 0.0003825922030955553\n","371 0.000362703634891659\n","372 0.00034381559817120433\n","373 0.00032587285386398435\n","374 0.00030880470876581967\n","375 0.0002926041488535702\n","376 0.00027721701189875603\n","377 0.0002626043278723955\n","378 0.0002487262536305934\n","379 0.00023555221559945494\n","380 0.0002230422687716782\n","381 0.0002111727517331019\n","382 0.00019990213331766427\n","383 0.00018921039009001106\n","384 0.0001790679234545678\n","385 0.00016944795788731426\n","386 0.00016031735867727548\n","387 0.00015166110824793577\n","388 0.0001434551231795922\n","389 0.00013566961570177227\n","390 0.00012829413753934205\n","391 0.00012130063987569883\n","392 0.00011467364674899727\n","393 0.00010839587775990367\n","394 0.00010244342411169782\n","395 9.680631046649069e-05\n","396 9.147007222054526e-05\n","397 8.641178283141926e-05\n","398 8.162091398844495e-05\n","399 7.708916382398456e-05\n","400 7.27963779354468e-05\n","401 6.87343199388124e-05\n","402 6.48887871648185e-05\n","403 6.12505100434646e-05\n","404 5.7808676501736045e-05\n","405 5.455052450997755e-05\n","406 5.1470786274876446e-05\n","407 4.855554652749561e-05\n","408 4.579980668495409e-05\n","409 4.320196603657678e-05\n","410 4.073501986567862e-05\n","411 3.840774661512114e-05\n","412 3.6207806260790676e-05\n","413 3.412776641198434e-05\n","414 3.216618642909452e-05\n","415 3.0310275178635493e-05\n","416 2.85601108771516e-05\n","417 2.690514702408109e-05\n","418 2.534244049456902e-05\n","419 2.3868093194323592e-05\n","420 2.247490920126438e-05\n","421 2.116056020895485e-05\n","422 1.9919645637855865e-05\n","423 1.8751083189272322e-05\n","424 1.7647431377554312e-05\n","425 1.660469024500344e-05\n","426 1.5622199498466216e-05\n","427 1.4695466234115884e-05\n","428 1.3822889741277322e-05\n","429 1.2999237696931232e-05\n","430 1.2222956684126984e-05\n","431 1.1492477824504022e-05\n","432 1.0802916222019121e-05\n","433 1.0153668881685007e-05\n","434 9.541529834677931e-06\n","435 8.965157576312777e-06\n","436 8.422613973380066e-06\n","437 7.911514330771752e-06\n","438 7.429958259308478e-06\n","439 6.977953034947859e-06\n","440 6.550949365191627e-06\n","441 6.149758064566413e-06\n","442 5.77266246182262e-06\n","443 5.417659394879593e-06\n","444 5.083582436782308e-06\n","445 4.769685347127961e-06\n","446 4.474276465771254e-06\n","447 4.196780537313316e-06\n","448 3.935170752811246e-06\n","449 3.6897945392411202e-06\n","450 3.4591214443935314e-06\n","451 3.2430202736577485e-06\n","452 3.038387831111322e-06\n","453 2.847818677764735e-06\n","454 2.6681277631723788e-06\n","455 2.499008814993431e-06\n","456 2.340848823223496e-06\n","457 2.1925995952187805e-06\n","458 2.052362788163009e-06\n","459 1.92153015632357e-06\n","460 1.7985543081522337e-06\n","461 1.683362484072859e-06\n","462 1.5750608781672781e-06\n","463 1.473872998758452e-06\n","464 1.3786659565084847e-06\n","465 1.2896531416117796e-06\n","466 1.2060430663041188e-06\n","467 1.1277168141532457e-06\n","468 1.0542785275902133e-06\n","469 9.85520273388829e-07\n","470 9.209943527821451e-07\n","471 8.605846346654289e-07\n","472 8.038372811824956e-07\n","473 7.511602007070906e-07\n","474 7.014601237642637e-07\n","475 6.548636406478181e-07\n","476 6.115482733548561e-07\n","477 5.709633796868729e-07\n","478 5.330174985829217e-07\n","479 4.973470026925497e-07\n","480 4.641086945866846e-07\n","481 4.3318598841324274e-07\n","482 4.040152816742193e-07\n","483 3.767854650504887e-07\n","484 3.511786133003625e-07\n","485 3.2749503020568227e-07\n","486 3.0526001637554145e-07\n","487 2.8450759259612823e-07\n","488 2.650921828717401e-07\n","489 2.4702143264221377e-07\n","490 2.3016647787699185e-07\n","491 2.1431040408970148e-07\n","492 1.9958214636517368e-07\n","493 1.8584847794045345e-07\n","494 1.7299568355610973e-07\n","495 1.6104306155284576e-07\n","496 1.4981297624672152e-07\n","497 1.395146256299995e-07\n","498 1.297108127573665e-07\n","499 1.2073938648882176e-07\n"],"name":"stdout"}]},{"metadata":{"id":"_pcgHuf-7sOo","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}