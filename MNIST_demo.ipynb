{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST handwritten digits classification with MLPs\n",
    "\n",
    "In this notebook, we'll train a multi-layer perceptron model to classify MNIST digits using PyTorch.\n",
    "\n",
    "First, the needed imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import progressbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Hyper-Parameter of the network\n",
    "They are often used in processes to help estimate model parameters.\n",
    "They are often specified by the practitioner.\n",
    "They can often be set using heuristics.\n",
    "They are often tuned for a given predictive modeling problem.\n",
    "    \n",
    "Hyperparameters are usually fixed before the actual training process begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The initial size of the First layer\n",
    "input_size = 784\n",
    "\n",
    "# The hidden state size\n",
    "hidden_size = 500\n",
    "\n",
    "# Number of different classes in MNIST classification task\n",
    "num_classes = 10\n",
    "\n",
    "# Total number of iteration/epochs we will be running our network\n",
    "num_epochs = 5\n",
    "\n",
    "# Number of example(batch_size) to be fed to the network at once\n",
    "batch_size = 100\n",
    "\n",
    "# The rate at which the network learns/unlearns\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Next we'll load the MNIST data. First time we may have to download the data, which can take a while.\n",
    "\n",
    "#### Data loader. \n",
    "Combines a dataset and a sampler, and provides single- or multi-process iterators over the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()),\n",
    "    batch_size=1, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train and test data are provided via data loaders that provide iterators over the datasets. \n",
    "\n",
    "The first element of training data (X_train) is a 4th-order tensor of size (batch_size, 1, 28, 28), i.e. it consists of a batch of images of size 1x28x28 pixels. \n",
    "\n",
    "y_train is a vector containing the correct classes (\"0\", \"1\", ..., \"9\") for each training digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('==>>> total trainning batch number: {}'.format(len(train_loader)))\n",
    "print ('==>>> total testing batch number: {}'.format(len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Here are the first 16 training digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images,_ = next(iter(train_loader))\n",
    "i = torchvision.utils.make_grid(images).numpy()\n",
    "i = np.transpose(i,(1,2,0))\n",
    "plt.imshow(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP network definition\n",
    "\n",
    "Let's define the network as a Python class. \n",
    "\n",
    "We have to write the __init__() and forward() methods, and PyTorch will automatically generate a backward() method for computing the gradients for the backward pass.\n",
    "\n",
    "### nn.linear()\n",
    "Applies a linear transformation to the incoming data: y=Ax+b\n",
    "\n",
    "Parameters:\t\n",
    "    in_features – size of each input sample\n",
    "    out_features – size of each output sample\n",
    "    bias – If set to False, the layer will not learn an additive bias. Default: True\n",
    "\n",
    "Variables:\t\n",
    "    weight – the learnable weights of the module of shape (out_features x in_features)\n",
    "    bias – the learnable bias of the module of shape (out_features)\n",
    "\n",
    "### nn.Relu()\n",
    "Applies the rectified linear unit function element-wise ReLU(x)=max(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(input_size, hidden_size, num_classes)\n",
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define an optimizer to update the model parameters based on the computed gradients. \n",
    "\n",
    "We select ADAM (with momentum) as the optimization algorithm, and set learning rate to 0.01.\n",
    "\n",
    "Note that there are several different options for the optimizer in PyTorch that we could use instead of ADAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_mlp = []\n",
    "train_accu_mlp = []\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  \n",
    "\n",
    "bar = progressbar.ProgressBar()\n",
    "for epoch in bar(range(num_epochs)):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Convert torch tensor to Variable\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        train_loss_mlp.append(loss.data[0])\n",
    "        optimizer.step()\n",
    "        \n",
    "        prediction = outputs.data.max(1)[1]   # first column has actual prob.\n",
    "        accuracy = prediction.eq(labels.data).sum()/batch_size*100\n",
    "        train_accu_mlp.append(accuracy)\n",
    "        \n",
    "        \n",
    "        if (i+1) % 500 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                   %(epoch+1, num_epochs, i+1, 3750//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss is a function of the difference of the network output and the target values. We are minimizing the loss function during training so it should decrease over time.\n",
    "Accuracy is the classification accuracy for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, 28*28))\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize how the training progressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(train_loss_mlp)), train_loss_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(train_accu_mlp)), train_accu_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let load the dataset and check the predictions of the network visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()),\n",
    "    batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take one example from the dataset at a time and predicting on it to see the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,target = next(iter(train_loader))\n",
    "\n",
    "i = torchvision.utils.make_grid(data).numpy()\n",
    "i = np.transpose(i,(1,2,0))\n",
    "plt.imshow(i)\n",
    "\n",
    "data, target = Variable(data.view(-1, 28*28), volatile=True), Variable(target)\n",
    "output = net(data)\n",
    "prediction = output.data.max(1)[1]\n",
    "correct += prediction.eq(target.data).sum()\n",
    "# print('\\nTest set: Accuracy: {:.2f}%'.format(100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST CNN\n",
    "\n",
    "Start with loading the data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()),\n",
    "    batch_size=100, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Conv2d\n",
    "\n",
    "    Applies a 2D convolution over an input signal composed of several input planes.\n",
    "    stride controls the stride for the cross-correlation, a single number or a tuple.\n",
    "    padding controls the amount of implicit zero-paddings on both sides for padding number of points for each dimension.\n",
    "    dilation controls the spacing between the kernel points; also known as the à trous algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MnistModel, self).__init__()\n",
    "        \n",
    "        # input is 28x28\n",
    "        # padding=2 for same padding\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5, padding=2)\n",
    "        \n",
    "        # feature map size is 14*14 by pooling\n",
    "        # padding=2 for same padding\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, padding=2)\n",
    "        \n",
    "        # feature map size is 7*7 by pooling\n",
    "        self.fc1 = nn.Linear(64*7*7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, 64*7*7)   # reshape Variable\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "model = MnistModel()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the size of the model parameter after each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization:\n",
    "Use the optim package to define an Optimizer that will update the weights of the model for us. \n",
    "\n",
    "Here we will use Adam; the optim package contains many otheroptimization algoriths. \n",
    "The first argument to the Adam constructor tells the optimizer which Variables it should update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network that we just defined\n",
    "\n",
    "#### Using the same steps for training:\n",
    "\n",
    "    Define the input and the target variable\n",
    "    Pass the data to the model for a forward pass\n",
    "    Manually zero out the optimizer \n",
    "    Calculate the loss\n",
    "    Backpropagate\n",
    "    Optimize\n",
    "    Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "train_loss = []\n",
    "train_accu = []\n",
    "i = 0\n",
    "bar = progressbar.ProgressBar()\n",
    "for epoch in bar(range(3)):\n",
    "    bar = progressbar.ProgressBar()\n",
    "    for data, target in bar(train_loader):\n",
    "        \n",
    "        # Define the input and the target variable\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        # Pass the data to the model for a forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = F.nll_loss(output, target)  # Calculate loss\n",
    "        loss.backward()    # calc gradients i.e backpropogation\n",
    "        \n",
    "        \n",
    "        train_loss.append(loss.data[0])\n",
    "        optimizer.step()   # update gradients\n",
    "        prediction = output.data.max(1)[1]   # first column has actual prob.\n",
    "        if i % 200 == 0:\n",
    "            print('Train Step: {}\\tLoss: {:.3f}'.format(i, loss.data[0]))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize how the training progressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(train_loss)), train_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Loss is a function of the difference of the network output and the target values. We are minimizing the loss function during training so it should decrease over time. Accuracy is the classification accuracy for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let load the dataset and check the predictions of the network visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()),\n",
    "    batch_size=1, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take one example at a time and predict on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,target = next(iter(train_loader))\n",
    "\n",
    "i = torchvision.utils.make_grid(data).numpy()\n",
    "i = np.transpose(i,(1,2,0))\n",
    "plt.imshow(i)\n",
    "\n",
    "data, target = Variable(data, volatile=True), Variable(target)\n",
    "output = model(data)\n",
    "prediction = output.data.max(1)[1]\n",
    "correct += prediction.eq(target.data).sum()\n",
    "# print('\\nTest set: Accuracy: {:.2f}%'.format(100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
